{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is inspired by [Ben Trevett notebook](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/1%20-%20Simple%20Sentiment%20Analysis.ipynb)\n",
    "\n",
    "Basically we'll be building a machine learning model to detect IT job (i.e. detect if a job posting is an IT job or not) using PyTorch and TorchText. This will be done on [Armenian Job posting dataset](https://www.kaggle.com/madhab/jobposts/home) published in Kaggle.\n",
    "\n",
    "In this first notebook, we'll start very simple to understand the general concepts whilst not really caring about good results. Further notebooks will build on this knowledge, to actually get good results.\n",
    "\n",
    "We'll be using a recurrent neural network (RNN) which reads a sequence of words, and for each word (sometimes called a step) will output a hidden state. We then use the hidden state for subsequent word in the sentence, until the final word has been fed into the RNN. This final hidden state will then be used to predict the sentiment of the sentence.\n",
    "\n",
    "![RNN Image](https://camo.githubusercontent.com/45a3950547d071988cea037b78c8183cdbed0b17/68747470733a2f2f692e696d6775722e636f6d2f566564593969472e706e67 \"RNN\")\n",
    "\n",
    "## Preparing Data\n",
    "\n",
    "One of the main concepts of TorchText is the Field. These define how your data should be processed. In our IT job classification task we have to sources of data, the raw string of the jobpost description and IT label, either \"True\" or \"False\".\n",
    "\n",
    "We use the TEXT field to handle the job description and the LABEL field to handle the IT label.\n",
    "\n",
    "The parameters of a Field specify how the data should be processed.\n",
    "\n",
    "Our TEXT field has `tokenize='spacy'`, which defines that the \"tokenization\" (the act of splitting the string into discrete \"tokens\") should be done using the spaCy tokenizer. If no tokenize argument is passed, the default is simply splitting the string on spaces.\n",
    "\n",
    "LABEL is defined by a LabelField, a special subset of the Field class specifically for handling labels. We will explain the tensor_type argument later.\n",
    "\n",
    "For more on Fields, go [here](https://github.com/pytorch/text/blob/master/torchtext/data/field.py).\n",
    "\n",
    "We also set the random seeds for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "\n",
    "TEXT = data.Field(tokenize='spacy')\n",
    "LABEL = data.LabelField(tensor_type=torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jobpost</th>\n",
       "      <th>date</th>\n",
       "      <th>Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>AnnouncementCode</th>\n",
       "      <th>Term</th>\n",
       "      <th>Eligibility</th>\n",
       "      <th>Audience</th>\n",
       "      <th>StartDate</th>\n",
       "      <th>Duration</th>\n",
       "      <th>...</th>\n",
       "      <th>Salary</th>\n",
       "      <th>ApplicationP</th>\n",
       "      <th>OpeningDate</th>\n",
       "      <th>Deadline</th>\n",
       "      <th>Notes</th>\n",
       "      <th>AboutC</th>\n",
       "      <th>Attach</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>IT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AMERIA Investment Consulting Company\\nJOB TITL...</td>\n",
       "      <td>Jan 5, 2004</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>AMERIA Investment Consulting Company</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>To apply for this position, please submit a\\nc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26 January 2004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2004</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>International Research &amp; Exchanges Board (IREX...</td>\n",
       "      <td>Jan 7, 2004</td>\n",
       "      <td>Full-time Community Connections Intern (paid i...</td>\n",
       "      <td>International Research &amp; Exchanges Board (IREX)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3 months</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Please submit a cover letter and resume to:\\nI...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12 January 2004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The International Research &amp; Exchanges Board (...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2004</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Caucasus Environmental NGO Network (CENN)\\nJOB...</td>\n",
       "      <td>Jan 7, 2004</td>\n",
       "      <td>Country Coordinator</td>\n",
       "      <td>Caucasus Environmental NGO Network (CENN)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Renewable annual contract\\nPOSITION</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Please send resume or CV toursula.kazarian@......</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20 January 2004\\nSTART DATE:  February 2004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Caucasus Environmental NGO Network is a\\nn...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2004</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Manoff Group\\nJOB TITLE:  BCC Specialist\\nPOSI...</td>\n",
       "      <td>Jan 7, 2004</td>\n",
       "      <td>BCC Specialist</td>\n",
       "      <td>Manoff Group</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Please send cover letter and resume to Amy\\nPe...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23 January 2004\\nSTART DATE:  Immediate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2004</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yerevan Brandy Company\\nJOB TITLE:  Software D...</td>\n",
       "      <td>Jan 10, 2004</td>\n",
       "      <td>Software Developer</td>\n",
       "      <td>Yerevan Brandy Company</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Successful candidates should submit\\n- CV; \\n-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20 January 2004, 18:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2004</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             jobpost          date  \\\n",
       "0  AMERIA Investment Consulting Company\\nJOB TITL...   Jan 5, 2004   \n",
       "1  International Research & Exchanges Board (IREX...   Jan 7, 2004   \n",
       "2  Caucasus Environmental NGO Network (CENN)\\nJOB...   Jan 7, 2004   \n",
       "3  Manoff Group\\nJOB TITLE:  BCC Specialist\\nPOSI...   Jan 7, 2004   \n",
       "4  Yerevan Brandy Company\\nJOB TITLE:  Software D...  Jan 10, 2004   \n",
       "\n",
       "                                               Title  \\\n",
       "0                            Chief Financial Officer   \n",
       "1  Full-time Community Connections Intern (paid i...   \n",
       "2                                Country Coordinator   \n",
       "3                                     BCC Specialist   \n",
       "4                                 Software Developer   \n",
       "\n",
       "                                           Company AnnouncementCode Term  \\\n",
       "0             AMERIA Investment Consulting Company              NaN  NaN   \n",
       "1  International Research & Exchanges Board (IREX)              NaN  NaN   \n",
       "2        Caucasus Environmental NGO Network (CENN)              NaN  NaN   \n",
       "3                                     Manoff Group              NaN  NaN   \n",
       "4                           Yerevan Brandy Company              NaN  NaN   \n",
       "\n",
       "  Eligibility Audience StartDate                             Duration  ...    \\\n",
       "0         NaN      NaN       NaN                                  NaN  ...     \n",
       "1         NaN      NaN       NaN                             3 months  ...     \n",
       "2         NaN      NaN       NaN  Renewable annual contract\\nPOSITION  ...     \n",
       "3         NaN      NaN       NaN                                  NaN  ...     \n",
       "4         NaN      NaN       NaN                                  NaN  ...     \n",
       "\n",
       "  Salary                                       ApplicationP OpeningDate  \\\n",
       "0    NaN  To apply for this position, please submit a\\nc...         NaN   \n",
       "1    NaN  Please submit a cover letter and resume to:\\nI...         NaN   \n",
       "2    NaN  Please send resume or CV toursula.kazarian@......         NaN   \n",
       "3    NaN  Please send cover letter and resume to Amy\\nPe...         NaN   \n",
       "4    NaN  Successful candidates should submit\\n- CV; \\n-...         NaN   \n",
       "\n",
       "                                      Deadline Notes  \\\n",
       "0                              26 January 2004   NaN   \n",
       "1                              12 January 2004   NaN   \n",
       "2  20 January 2004\\nSTART DATE:  February 2004   NaN   \n",
       "3      23 January 2004\\nSTART DATE:  Immediate   NaN   \n",
       "4                       20 January 2004, 18:00   NaN   \n",
       "\n",
       "                                              AboutC Attach  Year Month     IT  \n",
       "0                                                NaN    NaN  2004     1  False  \n",
       "1  The International Research & Exchanges Board (...    NaN  2004     1  False  \n",
       "2  The Caucasus Environmental NGO Network is a\\nn...    NaN  2004     1  False  \n",
       "3                                                NaN    NaN  2004     1  False  \n",
       "4                                                NaN    NaN  2004     1   True  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('../data/data job posts.csv')\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['jobpost', 'IT']].to_csv('../data/data_jobpost_it.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = data.TabularDataset(\n",
    "    path='../data/data_jobpost_it.csv', format='csv', \n",
    "    fields=[\n",
    "        ('text', TEXT), \n",
    "        ('label', LABEL)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = pos.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how many examples are in each split by checking their length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_data): 13301\n",
      "len(test_data): 5701\n"
     ]
    }
   ],
   "source": [
    "print('len(train_data):', len(train_data))\n",
    "print('len(test_data):', len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the fields of the data, hoping that it they match the Fields given earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data.fields: {'text': <torchtext.data.field.Field object at 0x7f6908977940>, 'label': <torchtext.data.field.LabelField object at 0x7f6908977c18>}\n"
     ]
    }
   ],
   "source": [
    "print('train_data.fields:', train_data.fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vars(train_data[0]): {'text': ['Aspid', 'Technologies', 'Co.', 'Ltd', '\\n', 'TITLE', ':', ' ', 'Helpdesk/', 'Administrative', 'Assistant', '\\n', 'TERM', ':', ' ', 'Full', 'time', 'or', 'Part', 'time', '\\n', 'START', 'DATE/', 'TIME', ':', ' ', 'Immediately', '\\n', 'DURATION', ':', ' ', 'Long', 'term', 'with', '2', 'months', 'probation', 'period', '\\n', 'LOCATION', ':', ' ', 'Yerevan', ',', 'Armenia', '\\n', 'JOB', 'DESCRIPTION', ':', ' ', 'N', '/', 'A', '\\n', 'JOB', 'RESPONSIBILITIES', ':', '\\n', '-', 'Provide', 'technical', 'assistance', 'to', 'the', 'company', \"'s\", 'global', 'customer', 'base', ';', '\\n', '-', 'Answer', ',', 'transfer', 'and', 'record', 'phone', 'calls', 'and', 'emails', ';', 'send', 'and', 'receive', '\\n', 'documents', 'via', 'fax', ',', 'post', 'offices', ';', '\\n', '-', 'Receive', 'and', 'control', 'visitors', ',', 'external', 'and', 'internal', 'people', ';', '\\n', '-', 'Check', 'internal', 'and', 'external', 'emails', ';', 'record', 'incoming', 'and', 'outgoing', 'mail', ';', '\\n', '-', 'Translate', 'materials', 'from', 'English', 'into', 'Armenian', 'languages', 'and', 'vice', '\\n', 'versa', 'as', 'needed', ';', '\\n', '-', 'Distribute', 'office', 'supply', 'and', 'stationery', ',', 'keep', 'records', ';', '\\n', '-', 'Assist', 'in', 'logistics', 'coordination', ',', 'including', 'hotel', 'accommodations', ',', '\\n', 'service', 'coordination', ';', '\\n', '-', 'Provide', 'assistance', 'with', 'the', 'filing', 'of', 'all', 'applicable', ',', 'relevant', '\\n', 'documents', ';', '\\n', '-', 'Other', 'duties', 'and', 'responsibilities', 'as', 'reasonably', 'requested', 'by', '\\n', 'management', '.', '\\n', 'REQUIRED', 'QUALIFICATIONS', ':', '\\n', '-', 'Excellent', 'written', 'and', 'verbal', 'communication', 'skills', 'in', 'English', 'and', '\\n', 'Armenian', 'languages', ';', '\\n', '-', 'Knowledge', 'of', 'MS', 'Office', ';', '\\n', '-', 'Technical', 'background/', 'education', 'preferable', ';', '\\n', '-', 'Patient', 'and', 'pleasant', 'disposition', ',', 'and', 'phone', 'manners', '.', '\\n', 'APPLICATION', 'PROCEDURES', ':', ' ', 'To', 'apply', ',', 'e', '-', 'mail', 'your', 'CV', 'indicating', 'the', 'job', '\\n', 'title', 'Admin', 'Assistant', '\"', 'in', 'the', 'subject', 'of', 'your', 'email', 'to', ':', 'cv@', '....', '\\n', 'Please', 'clearly', 'mention', 'in', 'your', 'application', 'letter', 'that', 'you', 'learned', 'of', '\\n', 'this', 'job', 'opportunity', 'through', 'Career', 'Center', 'and', 'mention', 'the', 'URL', 'of', 'its', '\\n', 'website', '-', 'www.careercenter.am', ',', 'Thanks', '.', '\\n', 'OPENING', 'DATE', ':', ' ', '20', 'June', '2007', '\\n', 'APPLICATION', 'DEADLINE', ':', ' ', '30', 'June', '2007', '\\n', 'ABOUT', 'COMPANY', ':', ' ', 'Aspid', 'Technologies', 'Co.', 'Ltd.', 'is', 'an', 'IT', 'company', 'based', 'in', '\\n', 'Yerevan', '.', '\\n', '----------------------------------', '\\n', 'To', 'place', 'a', 'free', 'posting', 'for', 'job', 'or', 'other', 'career', '-', 'related', 'opportunities', '\\n', 'available', 'in', 'your', 'organization', ',', 'just', 'go', 'to', 'the', 'www.careercenter.am', '\\n', 'website', 'and', 'follow', 'the', '\"', 'Post', 'an', 'Announcement', '\"', 'link', '.'], 'label': 'False'}\n"
     ]
    }
   ],
   "source": [
    "print('vars(train_data[0]):', vars(train_data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default this splits 70/30, however by passing a split_ratio argument, we can change the ratio of the split, i.e. a split_ratio of 0.8 would mean 80% of the examples make up the training set and 20% make up the validation set.\n",
    "\n",
    "We also pass our random seed to the random_state argument, ensuring that we get the same train/validation split each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "train_data, valid_data = train_data.split(random_state=random.seed(SEED))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we'll view how many examples are in each split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_data): 9311\n",
      "len(valid_data): 3990\n",
      "len(test_data): 5701\n"
     ]
    }
   ],
   "source": [
    "print('len(train_data):', len(train_data))\n",
    "print('len(valid_data):', len(valid_data))\n",
    "print('len(test_data):', len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have to build a vocabulary. This is a effectively a look up table where every unique word in your dictionary (every word that occurs in all of your examples) has a corresponding index (an integer).\n",
    "\n",
    "We do this as our machine learning model cannot operate on strings, only numbers. Each index is used to construct a one-hot vector for each word. A one-hot vector is a vector where all of the elements are 0, except one, which is 1, and dimensionality is the total number of unique words in your vocabulary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_data, max_size=50000)\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do we only build the vocabulary on the training set? When testing any machine learning system you do not want to look at the test set in any way. We do not include the validation set as we want it to reflect the test set as much as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(TEXT.vocab): 36206\n",
      "len(LABEL.vocab): 3\n"
     ]
    }
   ],
   "source": [
    "print('len(TEXT.vocab):', len(TEXT.vocab))\n",
    "print('len(LABEL.vocab):', len(LABEL.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we feed sentences into our model, we feed a batch of them at a time, i.e. more than one at a time, and all sentences in the batch need to be the same size. Thus, to ensure each sentence in the batch is the same size, any shorter than the largest within the batch are padded.\n",
    "\n",
    "We can also view the most common words in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('\\n', 470238), ('-', 193482), ('and', 164214), (',', 144175), ('the', 128180), (';', 124992), (':', 124759), ('of', 115034), ('.', 99890), (' ', 88374), ('in', 84923), ('to', 73901), ('for', 41879), ('a', 40413), ('with', 29454), ('\"', 28565), ('your', 25730), ('or', 23616), (')', 23221), ('is', 21895)]\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.freqs.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see the vocabulary directly using either the stoi (string to int) or itos (int to string) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<pad>', '\\n', '-', 'and', ',', 'the', ';', ':', 'of']\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.itos[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check the labels, ensuring 0 is for False and 1 is for True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function _default_unk_index at 0x7f68f06e0b70>, {'False': 0, 'True': 1, 'IT': 2})\n"
     ]
    }
   ],
   "source": [
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step of preparing the data is creating the iterators.\n",
    "\n",
    "`BucketIterator` first sorts of the examples using the `sort_key`, here we use the length of the sentences, and then partitions them into buckets. When the iterator is called it returns a batch of examples from the same bucket. This will return a batch of examples where each example is a similar length, minimizing the amount of padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size=BATCH_SIZE, \n",
    "    sort_key=lambda x: len(x.text), \n",
    "    repeat=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        embedded = self.embedding(x)\n",
    "            \n",
    "        output, hidden = self.rnn(embedded)\n",
    "            \n",
    "        assert torch.equal(output[-1,:,:], hidden.squeeze(0))\n",
    "        \n",
    "        return self.fc(hidden.squeeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create an instance of our RNN class.\n",
    "\n",
    "The input dimension is the dimension of the one-hot vectors, which is equal to the vocabulary size.\n",
    "\n",
    "The embedding dimension is the size of the dense word vectors, this is usually around the square root of the vocab size.\n",
    "\n",
    "The hidden dimension is the size of the hidden states, this is usually around 100-500 dimensions, but depends on the vocab size, embedding dimension and the complexity of the task.\n",
    "\n",
    "The output dimension is usually the number of classes, however in the case of only 2 classes the output value is between 0 and 1 and thus can be 1-dimensional, i.e. a single scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "\n",
    "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "\n",
    "Now we'll set up the training and then train the model.\n",
    "\n",
    "First, we'll create an optimizer. This is the algorithm we use to update the parameters of the module. Here, we'll use stochastic gradient descent (SGD). The first argument is the parameters will be updated by the optimizer, the second is the learning rate, i.e. how much we'll change the parameters by when we do an update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define our loss function. In PyTorch this is commonly called a criterion.\n",
    "\n",
    "The loss function here is binary cross entropy with logits.\n",
    "\n",
    "The prediction for each sentence is an unbound real number, as our labels are either 0 or 1, we want to restrict the number between 0 and 1, we do this using the sigmoid function\n",
    "\n",
    "We then calculate this bound scalar using binary cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our criterion function calculates the loss, however we have to write our function to calculate the accuracy.\n",
    "\n",
    "This function first feeds the predictions through a sigmoid layer, squashing the values between 0 and 1, we then round them to the nearest integer. This rounds any value greater than 0.5 to 1 (a positive sentiment).\n",
    "\n",
    "We then calculate how many rounded predictions equal the actual labels and average it across the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #rounding predictions to the closest integer\n",
    "    rounded_preds = torch.round(F.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() \n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, batch.label)\n",
    "        \n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then train the model through multiple epochs, an epoch being a complete pass through all examples in the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/anaconda3/envs/machinelearning/lib/python3.6/site-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/david/anaconda3/envs/machinelearning/lib/python3.6/site-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Train Loss: 0.549, Train Acc: 77.15%, Val. Loss: 0.510, Val. Acc: 80.13%\n",
      "Epoch: 02, Train Loss: 0.502, Train Acc: 80.21%, Val. Loss: 0.505, Val. Acc: 80.13%\n",
      "Epoch: 03, Train Loss: 0.502, Train Acc: 80.11%, Val. Loss: 0.504, Val. Acc: 80.13%\n",
      "Epoch: 04, Train Loss: 0.501, Train Acc: 80.18%, Val. Loss: 0.503, Val. Acc: 80.13%\n",
      "Epoch: 05, Train Loss: 0.500, Train Acc: 80.23%, Val. Loss: 0.502, Val. Acc: 80.13%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc*100:.2f}%, Val. Loss: {valid_loss:.3f}, Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed the loss is not really decreasing and the accuracy is poor. This is due to several issues with the model which we'll improve in the next notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the metric you actually care about, the test loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/anaconda3/envs/machinelearning/lib/python3.6/site-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n",
      "/home/david/anaconda3/envs/machinelearning/lib/python3.6/site-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.502, Test Acc: 80.19%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_binary(preds):\n",
    "    \"\"\"\n",
    "    Convert predicted torch array to either 0 or 1\n",
    "    \"\"\"\n",
    "\n",
    "    #rounding predictions to the closest integer\n",
    "    return torch.round(F.sigmoid(preds))\n",
    "\n",
    "def predict(model, iterator):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = [] \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = to_binary(model(batch.text).squeeze(1)).cpu().numpy()\n",
    "            all_predictions += predictions.tolist()\n",
    "            \n",
    "    return all_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the confusion matrix based on predicted and actual test label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/anaconda3/envs/machinelearning/lib/python3.6/site-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n",
      "/home/david/anaconda3/envs/machinelearning/lib/python3.6/site-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torchtext.data.dataset.Dataset at 0x7f688be4aba8>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predicted_labels = predict(model, test_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/anaconda3/envs/machinelearning/lib/python3.6/site-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[4572,    8],\n",
       "       [1121,    0]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "test_actual_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_iterator:\n",
    "        test_actual_labels += batch.label.cpu().numpy().tolist()\n",
    "        \n",
    "cf = confusion_matrix(test_actual_labels, test_predicted_labels)\n",
    "cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4580, 1121]), array([0, 1, 2]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_actual_labels_hist = np.histogram(test_actual_labels, bins=[0, 1, 2])\n",
    "test_actual_labels_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([5693,    8]), array([0, 1, 2]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predicted_labels_hist = np.histogram(test_predicted_labels, bins=[0, 1, 2])\n",
    "test_predicted_labels_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can finally save our simple RNN model for IT job classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/anaconda3/envs/machinelearning/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type RNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(model, './models/01_simple_rnn.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
